import matplotlib.pyplot as plt
import os
import streamlit as st
import japanize_matplotlib
import matplotlib.font_manager as font_manager
import pandas as pd
import numpy as np
from scipy import stats
import warnings
from typing import Optional, Dict, Any, Tuple
import requests
import json


def display_header():
    st.caption('Created by Dit-Lab.(Daiki Ito)')


def set_font():
    # font_path = '../ipaexg.ttf'  # pages ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ä¸€ã¤ä¸Šã‚’è¦‹ã‚‹
    font_path = os.path.join(os.path.dirname(__file__), '..', 'ipaexg.ttf')

    plt.rcParams['font.family'] = 'IPAexGothic'


def display_guide():
    st.markdown("""
    - [**æƒ…å ±æ¢ç©¶ã‚¹ãƒ†ãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰**](https://dit-lab.notion.site/612d9665350544aa97a2a8514a03c77c?v=85ad37a3275b4717a0033516b9cfd9cc)
    - [**ä¸­ã®äººã®ãƒšãƒ¼ã‚¸ï¼ˆDit-Lab.ï¼‰**](https://dit-lab.notion.site/Dit-Lab-da906d09d3cf42a19a011cf4bf25a673?pvs=4)
    """)


def display_link():
    st.header('ãƒªãƒ³ã‚¯')
    st.markdown("""
    - [**ä¸­ã®äººã®ãƒšãƒ¼ã‚¸ï¼ˆDit-Lab.ï¼‰**](https://dit-lab.notion.site/Dit-Lab-da906d09d3cf42a19a011cf4bf25a673?pvs=4)
    - [**é€²æ•°å¤‰æ›å­¦ç¿’ã‚¢ãƒ—ãƒª**](https://easy-base-converter.streamlit.app)
    - [**easyRSA**](https://easy-rsa.streamlit.app/)
    - [**easyAutoMLï¼ˆå›å¸°ï¼‰**](https://huggingface.co/spaces/itou-daiki/pycaret_datascience_streamlit)
    - [**pkl_predict_reg**](https://huggingface.co/spaces/itou-daiki/pkl_predict_reg)
    - [**éŸ³ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹**](https://audiovisualizationanalysis-bpeekdjwymuf6nkqcb4cqy.streamlit.app)
    - [**3D RGB Cube Visualizer**](https://boxplot-4-mysteams.streamlit.app)
    - [**ä¸Šãƒãƒ¼ã‚¯è§’åº¦è¨ˆç®—è£œåŠ©ãƒ„ãƒ¼ãƒ«**](https://sailing-mark-angle.streamlit.app)
    - [**Factor Score Calculator**](https://factor-score-calculator.streamlit.app/)
    - [**easy Excel Merge**](https://easy-xl-merge.streamlit.app)
    - [**ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ã“ã¡ã‚‰ã¾ã§**](https://forms.gle/G5sMYm7dNpz2FQtU9)
    - [**ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰ï¼ˆGitHubï¼‰**](https://github.com/itou-daiki/easy_stat)
    """)


def display_copyright():
    st.subheader('')
    st.write('ã”æ„è¦‹ãƒ»ã”è¦æœ›ã¯â†’', 'https://forms.gle/G5sMYm7dNpz2FQtU9', 'ã¾ã§')
    st.write('')
    st.subheader('Â© 2022-2025 Dit-Lab.(Daiki Ito). All Rights Reserved.')
    st.write('easyStat: Open Source for Ubiquitous Statistics')
    st.write('Democratizing data, everywhere.')
    st.write('')


def display_special_thanks():
    st.subheader('In collaboration with our esteemed contributors:')
    st.write('ãƒ»Toshiyuki')
    st.write('With heartfelt appreciation for their dedication and support.')


# ==========================================
# çµ±è¨ˆåˆå­¦è€…å‘ã‘å­¦ç¿’æ”¯æ´æ©Ÿèƒ½
# ==========================================

class StatisticalLearningAssistant:
    """çµ±è¨ˆåˆå­¦è€…å‘ã‘ã®å­¦ç¿’æ”¯æ´ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.learning_levels = {
            'beginner': 'åˆç´šè€…',
            'intermediate': 'ä¸­ç´šè€…', 
            'advanced': 'ä¸Šç´šè€…'
        }
    
    def show_concept_explanation(self, concept_key: str, level: str = 'beginner'):
        """çµ±è¨ˆæ¦‚å¿µã®èª¬æ˜ã‚’è¡¨ç¤º"""
        explanations = {
            'correlation': {
                'beginner': """
                ğŸ“Š **ç›¸é–¢åˆ†æã¨ã¯ï¼Ÿ**
                
                ç›¸é–¢åˆ†æã¯ã€2ã¤ã®å¤‰æ•°ãŒã©ã‚Œãã‚‰ã„é–¢ä¿‚ã—ã¦ã„ã‚‹ã‹ã‚’èª¿ã¹ã‚‹åˆ†ææ–¹æ³•ã§ã™ã€‚
                
                **ç›¸é–¢ä¿‚æ•°ã®èª­ã¿æ–¹ï¼š**
                - 1ã«è¿‘ã„ï¼šå¼·ã„æ­£ã®ç›¸é–¢ï¼ˆä¸€æ–¹ãŒå¢—ãˆã‚‹ã¨ä»–æ–¹ã‚‚å¢—ãˆã‚‹ï¼‰
                - 0ã«è¿‘ã„ï¼šç›¸é–¢ãªã—ï¼ˆé–¢ä¿‚ãŒè–„ã„ï¼‰
                - -1ã«è¿‘ã„ï¼šå¼·ã„è² ã®ç›¸é–¢ï¼ˆä¸€æ–¹ãŒå¢—ãˆã‚‹ã¨ä»–æ–¹ã¯æ¸›ã‚‹ï¼‰
                
                **ä¾‹ï¼š** å‹‰å¼·æ™‚é–“ã¨æˆç¸¾ã€èº«é•·ã¨ä½“é‡ãªã©
                """,
                'intermediate': """
                ğŸ“Š **ç›¸é–¢åˆ†æã®è©³ç´°**
                
                ãƒ”ã‚¢ã‚½ãƒ³ã®ç›¸é–¢ä¿‚æ•°ï¼ˆrï¼‰ã¯ç·šå½¢é–¢ä¿‚ã®å¼·ã•ã‚’æ¸¬å®šã—ã¾ã™ã€‚
                
                **è§£é‡ˆã®ç›®å®‰ï¼š**
                - |r| â‰¥ 0.7ï¼šå¼·ã„ç›¸é–¢
                - 0.3 â‰¤ |r| < 0.7ï¼šä¸­ç¨‹åº¦ã®ç›¸é–¢
                - |r| < 0.3ï¼šå¼±ã„ç›¸é–¢
                
                **æ³¨æ„ç‚¹ï¼š** ç›¸é–¢é–¢ä¿‚â‰ å› æœé–¢ä¿‚
                """,
                'advanced': """
                ğŸ“Š **ç›¸é–¢åˆ†æã®çµ±è¨ˆçš„è©³ç´°**
                
                - ãƒ”ã‚¢ã‚½ãƒ³ã®ç©ç‡ç›¸é–¢ä¿‚æ•°ï¼šr = Î£[(xi-xÌ„)(yi-È³)] / âˆš[Î£(xi-xÌ„)Â²Î£(yi-È³)Â²]
                - å‰ææ¡ä»¶ï¼šæ­£è¦åˆ†å¸ƒã€ç·šå½¢é–¢ä¿‚ã€ç­‰åˆ†æ•£æ€§
                - æœ‰æ„æ€§æ¤œå®šï¼št = râˆš(n-2)/âˆš(1-rÂ²)
                """
            },
            'ttest': {
                'beginner': """
                ğŸ“Š **tæ¤œå®šã¨ã¯ï¼Ÿ**
                
                2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã®å¹³å‡å€¤ã«é•ã„ãŒã‚ã‚‹ã‹ã‚’èª¿ã¹ã‚‹æ¤œå®šã§ã™ã€‚
                
                **ç¨®é¡ï¼š**
                - å¯¾å¿œãªã—ï¼šç•°ãªã‚‹äººãŸã¡ã‚’æ¯”è¼ƒï¼ˆä¾‹ï¼šç”·æ€§vså¥³æ€§ã®èº«é•·ï¼‰
                - å¯¾å¿œã‚ã‚Šï¼šåŒã˜äººã®å‰å¾Œã‚’æ¯”è¼ƒï¼ˆä¾‹ï¼šè–¬ã®æœç”¨å‰å¾Œï¼‰
                
                **på€¤ãŒ0.05æœªæº€ãªã‚‰ã€Œæœ‰æ„ãªå·®ãŒã‚ã‚‹ã€ã¨åˆ¤æ–­ã—ã¾ã™**
                """,
                'intermediate': """
                ğŸ“Š **tæ¤œå®šã®è©³ç´°**
                
                **å‰ææ¡ä»¶ï¼š**
                - ãƒ‡ãƒ¼ã‚¿ãŒæ­£è¦åˆ†å¸ƒã«å¾“ã†
                - å¯¾å¿œãªã—ã®å ´åˆï¼šç­‰åˆ†æ•£æ€§
                - ç‹¬ç«‹æ€§
                
                **åŠ¹æœé‡ï¼ˆCohen's dï¼‰ï¼š**
                - 0.2ï¼šå°ã•ã„åŠ¹æœ
                - 0.5ï¼šä¸­ç¨‹åº¦ã®åŠ¹æœ  
                - 0.8ï¼šå¤§ãã„åŠ¹æœ
                """,
                'advanced': """
                ğŸ“Š **tæ¤œå®šã®çµ±è¨ˆçš„è©³ç´°**
                
                - å¯¾å¿œãªã—ï¼št = (xÌ„â‚-xÌ„â‚‚) / SE_diff
                - å¯¾å¿œã‚ã‚Šï¼št = dÌ„ / (sd/âˆšn)
                - Welchã®tæ¤œå®šï¼šç­‰åˆ†æ•£æ€§ã‚’ä»®å®šã—ãªã„
                - è‡ªç”±åº¦ã®èª¿æ•´ãŒé‡è¦
                """
            }
        }
        
        if concept_key in explanations and level in explanations[concept_key]:
            st.info(explanations[concept_key][level])
    
    def check_learning_progress(self, analysis_type: str):
        """å­¦ç¿’é€²æ—ã‚’ãƒã‚§ãƒƒã‚¯"""
        if 'learning_progress' not in st.session_state:
            st.session_state.learning_progress = set()
        
        st.session_state.learning_progress.add(analysis_type)
        
        progress_count = len(st.session_state.learning_progress)
        total_analyses = 14  # ç·åˆ†ææ•°
        
        st.sidebar.success(f"ğŸ“š å­¦ç¿’é€²æ—: {progress_count}/{total_analyses} å®Œäº†")
        
        if progress_count >= total_analyses:
            st.balloons()
            st.success("ğŸ‰ ã™ã¹ã¦ã®åˆ†æã‚’ä½“é¨“ã—ã¾ã—ãŸï¼çµ±è¨ˆãƒã‚¹ã‚¿ãƒ¼ã§ã™ã­ï¼")


# ==========================================
# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
# ==========================================

class StatisticalValidator:
    """çµ±è¨ˆåˆ†æã®ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def safe_file_load(uploaded_file) -> Optional[pd.DataFrame]:
        """å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            if uploaded_file is None:
                return None
                
            file_extension = uploaded_file.name.split('.')[-1].lower()
            
            if file_extension == 'csv':
                # CSVã®å ´åˆã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•åˆ¤å®š
                try:
                    df = pd.read_csv(uploaded_file, encoding='utf-8')
                except UnicodeDecodeError:
                    uploaded_file.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
                    df = pd.read_csv(uploaded_file, encoding='shift_jis')
            elif file_extension in ['xlsx', 'xls']:
                df = pd.read_excel(uploaded_file)
            else:
                st.error("âš ï¸ å¯¾å¿œã—ã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚CSVã€Excel(.xlsx/.xls)ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
                return None
            
            # åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            if df.empty:
                st.error("âš ï¸ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                return None
            
            # åˆ—åã®æ—¥æœ¬èªå¯¾å¿œç¢ºèª
            if any(col.startswith('Unnamed:') for col in df.columns):
                st.warning("âš ï¸ åˆ—åãŒæ­£ã—ãèª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã®1è¡Œç›®ã«åˆ—åãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            
            return df
            
        except Exception as e:
            st.error(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            st.info("ğŸ’¡ **è§£æ±ºã®ãƒ’ãƒ³ãƒˆ:**\n- ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ãªã„ã‹ç¢ºèª\n- Excelå½¢å¼ã®å ´åˆã¯.xlsxã§ä¿å­˜\n- CSVã®å ´åˆã¯æ–‡å­—ã‚³ãƒ¼ãƒ‰(UTF-8)ã‚’ç¢ºèª")
            return None
    
    @staticmethod
    def validate_sample_size(data: pd.DataFrame, min_size: int = 3, analysis_type: str = "åˆ†æ") -> bool:
        """ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®æ¤œè¨¼"""
        if len(data) < min_size:
            st.error(f"âš ï¸ {analysis_type}ã«ã¯æœ€ä½{min_size}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(data)}ä»¶")
            st.info("ğŸ’¡ **è§£æ±ºæ–¹æ³•:** ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹ã‹ã€ä»–ã®åˆ†ææ‰‹æ³•ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
            return False
        return True
    
    @staticmethod
    def check_missing_values(data: pd.DataFrame, columns: list) -> Dict[str, Any]:
        """æ¬ æå€¤ã®ãƒã‚§ãƒƒã‚¯"""
        missing_info = {}
        for col in columns:
            if col in data.columns:
                missing_count = data[col].isnull().sum()
                missing_info[col] = missing_count
                
                if missing_count > 0:
                    st.warning(f"âš ï¸ å¤‰æ•°ã€Œ{col}ã€ã«{missing_count}ä»¶ã®æ¬ æå€¤ãŒã‚ã‚Šã¾ã™ã€‚")
        
        total_missing = sum(missing_info.values())
        if total_missing > 0:
            st.info("ğŸ’¡ **æ¬ æå€¤ã®å¯¾å‡¦æ³•:**\n- ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒšãƒ¼ã‚¸ã§æ¬ æå€¤ã‚’å‡¦ç†\n- æ¬ æå€¤ã®ã‚ã‚‹è¡Œã‚’é™¤å¤–\n- å¹³å‡å€¤ã‚„ä¸­å¤®å€¤ã§è£œå®Œ")
        
        return missing_info
    
    @staticmethod
    def validate_data_types(data: pd.DataFrame, columns: list, expected_type: str = 'numeric') -> bool:
        """ãƒ‡ãƒ¼ã‚¿å‹ã®æ¤œè¨¼"""
        valid = True
        for col in columns:
            if col in data.columns:
                if expected_type == 'numeric':
                    if not pd.api.types.is_numeric_dtype(data[col]):
                        st.error(f"âš ï¸ å¤‰æ•°ã€Œ{col}ã€ã¯æ•°å€¤å‹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ç¾åœ¨ã®å‹: {data[col].dtype}")
                        st.info("ğŸ’¡ **è§£æ±ºæ–¹æ³•:** ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒšãƒ¼ã‚¸ã§æ•°å€¤å‹ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚")
                        valid = False
        return valid
    
    @staticmethod
    def check_normality(data: pd.Series, alpha: float = 0.05) -> Dict[str, Any]:
        """æ­£è¦æ€§ã®æ¤œå®š"""
        if len(data.dropna()) < 3:
            return {'test': 'insufficient_data', 'p_value': None, 'is_normal': False}
        
        # Shapiro-Wilkæ¤œå®šï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã„å ´åˆï¼‰
        if len(data.dropna()) <= 50:
            stat, p_value = stats.shapiro(data.dropna())
            test_name = 'Shapiro-Wilk'
        else:
            # Kolmogorov-Smirnovæ¤œå®šï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„å ´åˆï¼‰
            stat, p_value = stats.kstest(data.dropna(), 'norm')
            test_name = 'Kolmogorov-Smirnov'
        
        is_normal = p_value > alpha
        return {
            'test': test_name,
            'statistic': stat,
            'p_value': p_value,
            'is_normal': is_normal
        }
    
    @staticmethod
    def check_equal_variances(group1: pd.Series, group2: pd.Series, alpha: float = 0.05) -> Dict[str, Any]:
        """ç­‰åˆ†æ•£æ€§ã®æ¤œå®šï¼ˆLeveneæ¤œå®šï¼‰"""
        try:
            stat, p_value = stats.levene(group1.dropna(), group2.dropna())
            equal_variances = p_value > alpha
            return {
                'statistic': stat,
                'p_value': p_value,
                'equal_variances': equal_variances
            }
        except Exception as e:
            return {'error': str(e), 'equal_variances': False}


# ==========================================
# çµæœè§£é‡ˆæ”¯æ´æ©Ÿèƒ½
# ==========================================

class ResultInterpreter:
    """çµ±è¨ˆçµæœã®è§£é‡ˆæ”¯æ´ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def interpret_correlation(r: float, p_value: float, alpha: float = 0.05) -> str:
        """ç›¸é–¢ä¿‚æ•°ã®è§£é‡ˆ"""
        # ç›¸é–¢ã®å¼·ã•
        abs_r = abs(r)
        if abs_r >= 0.7:
            strength = "å¼·ã„"
        elif abs_r >= 0.3:
            strength = "ä¸­ç¨‹åº¦ã®"
        else:
            strength = "å¼±ã„"
        
        # ç›¸é–¢ã®æ–¹å‘
        direction = "æ­£ã®" if r > 0 else "è² ã®"
        
        # æœ‰æ„æ€§
        significance = "çµ±è¨ˆçš„ã«æœ‰æ„" if p_value < alpha else "çµ±è¨ˆçš„ã«æœ‰æ„ã§ã¯ãªã„"
        
        interpretation = f"""
        ğŸ“Š **çµæœã®è§£é‡ˆ**
        
        - **ç›¸é–¢ã®å¼·ã•**: {strength}ç›¸é–¢ (r = {r:.3f})
        - **ç›¸é–¢ã®æ–¹å‘**: {direction}ç›¸é–¢
        - **çµ±è¨ˆçš„æœ‰æ„æ€§**: {significance} (p = {p_value:.3f})
        
        **å®Ÿéš›ã®æ„å‘³**:
        """
        
        if abs_r >= 0.7 and p_value < alpha:
            interpretation += "2ã¤ã®å¤‰æ•°ã«ã¯å¼·ã„é–¢ä¿‚ãŒã‚ã‚Šã¾ã™ã€‚ä¸€æ–¹ãŒå¤‰åŒ–ã™ã‚‹ã¨ä»–æ–¹ã‚‚äºˆæ¸¬å¯èƒ½ãªå½¢ã§å¤‰åŒ–ã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚"
        elif abs_r >= 0.3 and p_value < alpha:
            interpretation += "2ã¤ã®å¤‰æ•°ã«ã¯ã‚ã‚‹ç¨‹åº¦ã®é–¢ä¿‚ãŒã‚ã‚Šã¾ã™ã€‚å®Œå…¨ã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€é–¢é€£æ€§ãŒèªã‚ã‚‰ã‚Œã¾ã™ã€‚"
        elif p_value >= alpha:
            interpretation += "çµ±è¨ˆçš„ã«æœ‰æ„ãªé–¢ä¿‚ã¯èªã‚ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚å¶ç„¶ã«ã‚ˆã‚‹çµæœã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
        else:
            interpretation += "é–¢ä¿‚ã¯ã‚ã‚‹ã‚‚ã®ã®å¼±ãã€å®Ÿéš›ã®äºˆæ¸¬ã‚„åˆ¤æ–­ã«ã¯æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚"
        
        return interpretation
    
    @staticmethod
    def interpret_ttest(t_stat: float, p_value: float, effect_size: float, alpha: float = 0.05) -> str:
        """tæ¤œå®šçµæœã®è§£é‡ˆ"""
        significance = "çµ±è¨ˆçš„ã«æœ‰æ„" if p_value < alpha else "çµ±è¨ˆçš„ã«æœ‰æ„ã§ã¯ãªã„"
        
        # åŠ¹æœé‡ã®è§£é‡ˆ
        if abs(effect_size) >= 0.8:
            effect_interpretation = "å¤§ãã„åŠ¹æœ"
        elif abs(effect_size) >= 0.5:
            effect_interpretation = "ä¸­ç¨‹åº¦ã®åŠ¹æœ"
        elif abs(effect_size) >= 0.2:
            effect_interpretation = "å°ã•ã„åŠ¹æœ"
        else:
            effect_interpretation = "åŠ¹æœã¯ã»ã¨ã‚“ã©ãªã„"
        
        interpretation = f"""
        ğŸ“Š **çµæœã®è§£é‡ˆ**
        
        - **çµ±è¨ˆçš„æœ‰æ„æ€§**: {significance} (p = {p_value:.3f})
        - **åŠ¹æœé‡**: {effect_interpretation} (Cohen's d = {effect_size:.3f})
        - **tçµ±è¨ˆé‡**: {t_stat:.3f}
        
        **çµè«–**:
        """
        
        if p_value < alpha:
            interpretation += f"2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—é–“ã«{effect_interpretation}ãªå·®ãŒã‚ã‚‹ã“ã¨ãŒçµ±è¨ˆçš„ã«ç¢ºèªã•ã‚Œã¾ã—ãŸã€‚"
        else:
            interpretation += "2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—é–“ã«çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ã¯èªã‚ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        return interpretation


# ==========================================
# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å­¦ç¿’ã‚¬ã‚¤ãƒ‰
# ==========================================

def show_interactive_guide(analysis_type: str):
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªå­¦ç¿’ã‚¬ã‚¤ãƒ‰ã‚’è¡¨ç¤º"""
    guides = {
        'correlation': {
            'title': 'ğŸ” ç›¸é–¢åˆ†æå­¦ç¿’ã‚¬ã‚¤ãƒ‰',
            'steps': [
                "1ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ã‚‡ã†",
                "2ï¸âƒ£ åˆ†æã—ãŸã„2ã¤ã®å¤‰æ•°ã‚’é¸æŠã—ã¾ã™",
                "3ï¸âƒ£ æ•£å¸ƒå›³ã§è¦–è¦šçš„ã«é–¢ä¿‚ã‚’ç¢ºèª",
                "4ï¸âƒ£ ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—ãƒ»è§£é‡ˆ",
                "5ï¸âƒ£ çµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆã«ã¾ã¨ã‚ã‚‹"
            ],
            'tips': [
                "ğŸ’¡ ã¾ãšã¯æ•£å¸ƒå›³ã§ç›´ç·šçš„ãªé–¢ä¿‚ãŒã‚ã‚‹ã‹ç¢ºèª",
                "ğŸ’¡ å¤–ã‚Œå€¤ãŒãªã„ã‹ãƒã‚§ãƒƒã‚¯",
                "ğŸ’¡ ç›¸é–¢â‰ å› æœé–¢ä¿‚ã‚’å¿˜ã‚Œãšã«"
            ]
        },
        'ttest': {
            'title': 'ğŸ“Š tæ¤œå®šå­¦ç¿’ã‚¬ã‚¤ãƒ‰',
            'steps': [
                "1ï¸âƒ£ æ¯”è¼ƒã—ãŸã„ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ˜ç¢ºã«ã™ã‚‹",
                "2ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦æ€§ã‚’ç¢ºèª",
                "3ï¸âƒ£ ç­‰åˆ†æ•£æ€§ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå¯¾å¿œãªã—ã®å ´åˆï¼‰",
                "4ï¸âƒ£ é©åˆ‡ãªtæ¤œå®šã‚’å®Ÿè¡Œ",
                "5ï¸âƒ£ åŠ¹æœé‡ã‚‚å«ã‚ã¦è§£é‡ˆ"
            ],
            'tips': [
                "ğŸ’¡ å¯¾å¿œã‚ã‚Šãƒ»ãªã—ã®é¸æŠãŒé‡è¦",
                "ğŸ’¡ på€¤ã ã‘ã§ãªãåŠ¹æœé‡ã‚‚ç¢ºèª",
                "ğŸ’¡ å®Ÿéš›çš„ãªæ„å‘³ã‚’è€ƒãˆã‚‹"
            ]
        }
    }
    
    if analysis_type in guides:
        guide = guides[analysis_type]
        
        with st.expander(f"{guide['title']} - ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹"):
            st.markdown("### ğŸ“‹ å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—")
            for step in guide['steps']:
                st.markdown(step)
            
            st.markdown("### ğŸ’¡ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ")
            for tip in guide['tips']:
                st.markdown(tip)
            
            # å­¦ç¿’ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
            st.markdown("### âœ… å­¦ç¿’ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ")
            for i, step in enumerate(guide['steps']):
                completed = st.checkbox(f"å®Œäº†: {step}", key=f"checklist_{analysis_type}_{i}")


def show_beginner_tips():
    """åˆå­¦è€…å‘ã‘ã®ãƒ’ãƒ³ãƒˆã‚’è¡¨ç¤º"""
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“ çµ±è¨ˆå­¦ç¿’ã®ã‚³ãƒ„")
    
    tips = [
        "ğŸ“Š ã¾ãšã¯ã‚°ãƒ©ãƒ•ã§è¦–è¦šåŒ–",
        "ğŸ”¢ æ•°å€¤ã ã‘ã§ãªãæ„å‘³ã‚‚è€ƒãˆã‚‹", 
        "â“ ã€Œãªãœï¼Ÿã€ã‚’å¸¸ã«å•ã„ã‹ã‘ã‚‹",
        "ğŸ“ˆ è¤‡æ•°ã®åˆ†æã‚’çµ„ã¿åˆã‚ã›ã‚‹",
        "ğŸ“ çµæœã‚’æ–‡ç« ã§èª¬æ˜ã—ã¦ã¿ã‚‹"
    ]
    
    for tip in tips:
        st.sidebar.markdown(f"- {tip}")


def create_learning_dashboard():
    """å­¦ç¿’ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ä½œæˆ"""
    if 'learning_progress' not in st.session_state:
        st.session_state.learning_progress = set()
    
    progress = st.session_state.learning_progress
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“š å­¦ç¿’é€²æ—ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    
    analyses = [
        "ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°", "EDA", "ç›¸é–¢åˆ†æ", "ã‚«ã‚¤äºŒä¹—æ¤œå®š",
        "tæ¤œå®šï¼ˆå¯¾å¿œãªã—ï¼‰", "tæ¤œå®šï¼ˆå¯¾å¿œã‚ã‚Šï¼‰", "ä¸€è¦å› åˆ†æ•£åˆ†æï¼ˆå¯¾å¿œãªã—ï¼‰",
        "ä¸€è¦å› åˆ†æ•£åˆ†æï¼ˆå¯¾å¿œã‚ã‚Šï¼‰", "äºŒè¦å› åˆ†æ•£åˆ†æ", "äºŒè¦å› æ··åˆåˆ†æ•£åˆ†æ",
        "å˜å›å¸°åˆ†æ", "é‡å›å¸°åˆ†æ", "å› å­åˆ†æ", "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°"
    ]
    
    completed_count = len(progress)
    total_count = len(analyses)
    progress_percentage = (completed_count / total_count) * 100
    
    st.sidebar.progress(progress_percentage / 100)
    st.sidebar.markdown(f"**é€²æ—: {completed_count}/{total_count} ({progress_percentage:.1f}%)**")
    
    if completed_count == total_count:
        st.sidebar.success("ğŸ† å…¨åˆ†æåˆ¶è¦‡ï¼")
    elif completed_count >= total_count // 2:
        st.sidebar.info("ğŸ“ˆ ä¸­ç´šè€…ãƒ¬ãƒ™ãƒ«é”æˆï¼")
    elif completed_count >= 3:
        st.sidebar.info("ğŸŒ± é †èª¿ã«å­¦ç¿’ä¸­ï¼")


# ==========================================
# ç”ŸæˆAIçµ±è¨ˆè§£é‡ˆæ”¯æ´æ©Ÿèƒ½
# ==========================================

class AIStatisticalInterpreter:
    """ç”ŸæˆAIã«ã‚ˆã‚‹çµ±è¨ˆè§£é‡ˆæ”¯æ´ã‚¯ãƒ©ã‚¹"""

    @staticmethod
    def call_gemini_api(api_key: str, prompt: str) -> str:
        """Gemini 2.0 Flash APIã‚’å‘¼ã³å‡ºã™é–¢æ•°"""
        if not api_key:
            return "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"

        url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent"
        headers = {
            "Content-Type": "application/json",
        }

        data = {
            "contents": [{
                "parts": [{
                    "text": prompt
                }]
            }],
            "generationConfig": {
                "temperature": 0.3,
                "maxOutputTokens": 2048,
            }
        }

        try:
            response = requests.post(f"{url}?key={api_key}", headers=headers, json=data)
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and len(result['candidates']) > 0:
                    return result['candidates'][0]['content']['parts'][0]['text']
                else:
                    return "APIã‹ã‚‰ã®å¿œç­”ãŒäºˆæœŸã—ãªã„å½¢å¼ã§ã™ã€‚"
            else:
                return f"APIã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}"
        except Exception as e:
            return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

    @staticmethod
    def setup_ai_sidebar() -> Tuple[str, bool]:
        """AIæ©Ÿèƒ½ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š"""
        st.sidebar.subheader("ğŸ¤– AIçµ±è¨ˆè§£é‡ˆæ©Ÿèƒ½")
        st.sidebar.write("Gemini 2.0 Flash APIã‚’ä½¿ç”¨ã—ã¦çµ±è¨ˆçµæœã‚’è‡ªå‹•è§£é‡ˆã—ã¾ã™")
        gemini_api_key = st.sidebar.text_input(
            "Gemini APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            type="password",
            help="Google AI Studio (https://aistudio.google.com/) ã§APIã‚­ãƒ¼ã‚’å–å¾—ã§ãã¾ã™"
        )
        enable_ai_interpretation = st.sidebar.checkbox(
            "AIè§£é‡ˆæ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹",
            disabled=not gemini_api_key
        )

        if gemini_api_key and enable_ai_interpretation:
            st.sidebar.success("âœ… AIè§£é‡ˆæ©Ÿèƒ½ãŒæœ‰åŠ¹ã«ãªã‚Šã¾ã—ãŸ")
        elif enable_ai_interpretation and not gemini_api_key:
            st.sidebar.error("âŒ APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

        return gemini_api_key, enable_ai_interpretation

    @staticmethod
    def create_correlation_interpretation_prompt(correlation_results: Dict[str, Any]) -> str:
        """ç›¸é–¢åˆ†æã®è§£é‡ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        r = correlation_results.get('correlation', 0)
        p_value = correlation_results.get('p_value', 1)
        var1 = correlation_results.get('var1', 'å¤‰æ•°1')
        var2 = correlation_results.get('var2', 'å¤‰æ•°2')
        n = correlation_results.get('sample_size', 0)

        prompt = f"""
ã‚ãªãŸã¯çµ±è¨ˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ç›¸é–¢åˆ†æã®çµæœã‚’è©³ç´°ã«è§£é‡ˆãƒ»è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚

ã€åˆ†æçµæœã€‘
- å¤‰æ•°1: {var1}
- å¤‰æ•°2: {var2}
- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {n}
- ç›¸é–¢ä¿‚æ•° (r): {r:.4f}
- på€¤: {p_value:.4f}

ã€è§£é‡ˆãƒ»è€ƒå¯Ÿã—ã¦ã»ã—ã„å†…å®¹ã€‘
1. ç›¸é–¢ä¿‚æ•°ã®å€¤ã‹ã‚‰è¦‹ãŸé–¢ä¿‚ã®å¼·ã•ã¨æ–¹å‘æ€§
   - æ•°å€¤ã®å…·ä½“çš„ãªæ„å‘³
   - ç›¸é–¢ã®å¼·åº¦ã®è©•ä¾¡ï¼ˆå¼±ã„/ä¸­ç¨‹åº¦/å¼·ã„ï¼‰

2. çµ±è¨ˆçš„æœ‰æ„æ€§ã®åˆ¤æ–­
   - på€¤ã®è§£é‡ˆ
   - å¸°ç„¡ä»®èª¬ã¨å¯¾ç«‹ä»®èª¬ã®åˆ¤æ–­

3. å®Ÿç”¨çš„ãªè§£é‡ˆ
   - ãƒ“ã‚¸ãƒã‚¹ã‚„ç ”ç©¶æ–‡è„ˆã§ã®æ„å‘³
   - å®Ÿéš›çš„ãªå½±éŸ¿åº¦ã®è©•ä¾¡

4. æ³¨æ„ç‚¹ã¨é™ç•Œ
   - ç›¸é–¢ã¨å› æœã®åŒºåˆ¥
   - ç¬¬ä¸‰ã®å¤‰æ•°ã®å¯èƒ½æ€§
   - ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®å¦¥å½“æ€§

5. ä»Šå¾Œã®åˆ†æææ¡ˆ
   - è¿½åŠ ã§å®Ÿæ–½ã™ã¹ãåˆ†æ
   - æ·±æ˜ã‚Šã™ã¹ããƒã‚¤ãƒ³ãƒˆ

çµ±è¨ˆã®å°‚é–€çŸ¥è­˜ãŒãªã„äººã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãã€å®Ÿè·µçš„ãªè§£é‡ˆã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚
"""
        return prompt

    @staticmethod
    def create_chi_square_interpretation_prompt(chi_square_results: Dict[str, Any]) -> str:
        """ã‚«ã‚¤äºŒä¹—æ¤œå®šã®è§£é‡ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        chi2 = chi_square_results.get('chi2', 0)
        p_value = chi_square_results.get('p_value', 1)
        dof = chi_square_results.get('dof', 0)
        var1 = chi_square_results.get('var1', 'å¤‰æ•°1')
        var2 = chi_square_results.get('var2', 'å¤‰æ•°2')
        crosstab = chi_square_results.get('crosstab', None)
        expected = chi_square_results.get('expected', None)

        crosstab_str = crosstab.to_string() if crosstab is not None else "ãƒ‡ãƒ¼ã‚¿ãªã—"
        expected_str = expected.to_string() if expected is not None else "ãƒ‡ãƒ¼ã‚¿ãªã—"

        prompt = f"""
ã‚ãªãŸã¯çµ±è¨ˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ã‚«ã‚¤äºŒä¹—æ¤œå®šã®çµæœã‚’è©³ç´°ã«è§£é‡ˆãƒ»è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚

ã€åˆ†æçµæœã€‘
- å¤‰æ•°1: {var1}
- å¤‰æ•°2: {var2}
- ã‚«ã‚¤äºŒä¹—çµ±è¨ˆé‡: {chi2:.4f}
- è‡ªç”±åº¦: {dof}
- på€¤: {p_value:.4f}

ã€è¦³æ¸¬åº¦æ•°ï¼ˆã‚¯ãƒ­ã‚¹è¡¨ï¼‰ã€‘
{crosstab_str}

ã€æœŸå¾…åº¦æ•°ã€‘
{expected_str}

ã€è§£é‡ˆãƒ»è€ƒå¯Ÿã—ã¦ã»ã—ã„å†…å®¹ã€‘
1. ã‚¯ãƒ­ã‚¹è¡¨ã®èª­ã¿å–ã‚Š
   - è¦³æ¸¬åº¦æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
   - é¡•è‘—ãªåã‚Šã®ç‰¹å®š

2. ã‚«ã‚¤äºŒä¹—çµ±è¨ˆé‡ã¨på€¤ã®è§£é‡ˆ
   - çµ±è¨ˆçš„æœ‰æ„æ€§ã®åˆ¤æ–­
   - å®Ÿéš›çš„ãªé–¢é€£ã®å¼·ã•

3. æœŸå¾…åº¦æ•°ã¨ã®å·®ç•°åˆ†æ
   - ã©ã®ã‚»ãƒ«ã§å¤§ããªå·®ãŒã‚ã‚‹ã‹
   - ãã®å®Ÿéš›çš„ãªæ„å‘³

4. å¤‰æ•°é–“ã®é–¢é€£æ€§ã®è§£é‡ˆ
   - ç‹¬ç«‹æ€§ã®æ¤œå®šçµæœ
   - é–¢é€£ã®ãƒ‘ã‚¿ãƒ¼ãƒ³

5. å®Ÿç”¨çš„ãªç¤ºå”†
   - ãƒ“ã‚¸ãƒã‚¹ãƒ»ç ”ç©¶ã§ã®æ´»ç”¨
   - æ„æ€æ±ºå®šã¸ã®å¿œç”¨

6. æ³¨æ„ç‚¹ã¨é™ç•Œ
   - æœŸå¾…åº¦æ•°ã®å¦¥å½“æ€§
   - å› æœé–¢ä¿‚ã®è§£é‡ˆåˆ¶é™

çµ±è¨ˆã®å°‚é–€çŸ¥è­˜ãŒãªã„äººã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãã€å®Ÿè·µçš„ãªè§£é‡ˆã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚
"""
        return prompt

    @staticmethod
    def create_ttest_interpretation_prompt(ttest_results: Dict[str, Any]) -> str:
        """tæ¤œå®šã®è§£é‡ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        t_stat = ttest_results.get('t_statistic', 0)
        p_value = ttest_results.get('p_value', 1)
        dof = ttest_results.get('dof', 0)
        mean1 = ttest_results.get('mean1', 0)
        mean2 = ttest_results.get('mean2', 0)
        std1 = ttest_results.get('std1', 0)
        std2 = ttest_results.get('std2', 0)
        n1 = ttest_results.get('n1', 0)
        n2 = ttest_results.get('n2', 0)
        effect_size = ttest_results.get('effect_size', 0)
        test_type = ttest_results.get('test_type', 'tæ¤œå®š')
        group1_name = ttest_results.get('group1_name', 'ã‚°ãƒ«ãƒ¼ãƒ—1')
        group2_name = ttest_results.get('group2_name', 'ã‚°ãƒ«ãƒ¼ãƒ—2')

        prompt = f"""
ã‚ãªãŸã¯çµ±è¨ˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®{test_type}ã®çµæœã‚’è©³ç´°ã«è§£é‡ˆãƒ»è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚

ã€åˆ†æçµæœã€‘
- æ¤œå®šã‚¿ã‚¤ãƒ—: {test_type}
- ã‚°ãƒ«ãƒ¼ãƒ—1 ({group1_name}): å¹³å‡={mean1:.4f}, æ¨™æº–åå·®={std1:.4f}, n={n1}
- ã‚°ãƒ«ãƒ¼ãƒ—2 ({group2_name}): å¹³å‡={mean2:.4f}, æ¨™æº–åå·®={std2:.4f}, n={n2}
- tçµ±è¨ˆé‡: {t_stat:.4f}
- è‡ªç”±åº¦: {dof}
- på€¤: {p_value:.4f}
- åŠ¹æœé‡ (Cohen's d): {effect_size:.4f}

ã€è§£é‡ˆãƒ»è€ƒå¯Ÿã—ã¦ã»ã—ã„å†…å®¹ã€‘
1. è¨˜è¿°çµ±è¨ˆã®æ¯”è¼ƒ
   - å¹³å‡å€¤ã®å·®ã®å®Ÿéš›çš„ãªå¤§ãã•
   - æ¨™æº–åå·®ã‹ã‚‰è¦‹ã‚‹ã°ã‚‰ã¤ã

2. tçµ±è¨ˆé‡ã¨på€¤ã®è§£é‡ˆ
   - çµ±è¨ˆçš„æœ‰æ„æ€§ã®åˆ¤æ–­
   - å¸°ç„¡ä»®èª¬ã®æ¡æŠ/æ£„å´

3. åŠ¹æœé‡ã®è©•ä¾¡
   - Cohen's dã®è§£é‡ˆï¼ˆå°/ä¸­/å¤§ï¼‰
   - å®Ÿéš›çš„ãªæ„å‘³ã§ã®å·®ã®å¤§ãã•

4. ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®å·®ã®å®Ÿç”¨çš„è§£é‡ˆ
   - ãƒ“ã‚¸ãƒã‚¹ãƒ»ç ”ç©¶ã§ã®æ„å‘³
   - æ„æ€æ±ºå®šã¸ã®ç¤ºå”†

5. ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®å¦¥å½“æ€§
   - çµ±è¨ˆçš„æ¤œå‡ºåŠ›ã®è©•ä¾¡
   - çµæœã®ä¿¡é ¼æ€§

6. æ³¨æ„ç‚¹ã¨é™ç•Œ
   - å‰ææ¡ä»¶ã®ç¢ºèª
   - å› æœæ¨è«–ã®åˆ¶é™
   - è¿½åŠ åˆ†æã®ææ¡ˆ

çµ±è¨ˆã®å°‚é–€çŸ¥è­˜ãŒãªã„äººã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãã€å®Ÿè·µçš„ãªè§£é‡ˆã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚
"""
        return prompt

    @staticmethod
    def create_anova_interpretation_prompt(anova_results: Dict[str, Any]) -> str:
        """åˆ†æ•£åˆ†æã®è§£é‡ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        f_stat = anova_results.get('f_statistic', 0)
        p_value = anova_results.get('p_value', 1)
        df_between = anova_results.get('df_between', 0)
        df_within = anova_results.get('df_within', 0)
        group_means = anova_results.get('group_means', {})
        eta_squared = anova_results.get('eta_squared', 0)
        analysis_type = anova_results.get('analysis_type', 'åˆ†æ•£åˆ†æ')

        means_str = "\n".join([f"- {group}: å¹³å‡={mean:.4f}" for group, mean in group_means.items()])

        prompt = f"""
ã‚ãªãŸã¯çµ±è¨ˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®{analysis_type}ã®çµæœã‚’è©³ç´°ã«è§£é‡ˆãƒ»è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚

ã€åˆ†æçµæœã€‘
- åˆ†æã‚¿ã‚¤ãƒ—: {analysis_type}
- Fçµ±è¨ˆé‡: {f_stat:.4f}
- è‡ªç”±åº¦: ç¾¤é–“={df_between}, ç¾¤å†…={df_within}
- på€¤: {p_value:.4f}
- åŠ¹æœé‡ (Î·Â²): {eta_squared:.4f}

ã€å„ã‚°ãƒ«ãƒ¼ãƒ—ã®å¹³å‡å€¤ã€‘
{means_str}

ã€è§£é‡ˆãƒ»è€ƒå¯Ÿã—ã¦ã»ã—ã„å†…å®¹ã€‘
1. è¨˜è¿°çµ±è¨ˆã®æ¯”è¼ƒ
   - å„ã‚°ãƒ«ãƒ¼ãƒ—ã®å¹³å‡å€¤ã®å‚¾å‘
   - æœ€å¤§ãƒ»æœ€å°ã®å·®

2. Fçµ±è¨ˆé‡ã¨på€¤ã®è§£é‡ˆ
   - çµ±è¨ˆçš„æœ‰æ„æ€§ã®åˆ¤æ–­
   - ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®å·®ã®å­˜åœ¨

3. åŠ¹æœé‡ã®è©•ä¾¡
   - Î·Â²ã®è§£é‡ˆ
   - å®Ÿéš›çš„ãªå·®ã®å¤§ãã•

4. å¤šé‡æ¯”è¼ƒã®å¿…è¦æ€§
   - ã©ã®ã‚°ãƒ«ãƒ¼ãƒ—é–“ã§å·®ãŒã‚ã‚‹ã‹
   - äº‹å¾Œæ¤œå®šã®æ¨å¥¨

5. å®Ÿç”¨çš„ãªç¤ºå”†
   - ãƒ“ã‚¸ãƒã‚¹ãƒ»ç ”ç©¶ã§ã®æ„å‘³
   - æœ€é©ãªã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹å®š

6. æ³¨æ„ç‚¹ã¨é™ç•Œ
   - å‰ææ¡ä»¶ã®ç¢ºèª
   - Type Iã‚¨ãƒ©ãƒ¼ã®åˆ¶å¾¡
   - è¿½åŠ åˆ†æã®ææ¡ˆ

çµ±è¨ˆã®å°‚é–€çŸ¥è­˜ãŒãªã„äººã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãã€å®Ÿè·µçš„ãªè§£é‡ˆã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚
"""
        return prompt

    @staticmethod
    def display_ai_interpretation(
        api_key: str,
        enabled: bool,
        results: Dict[str, Any],
        analysis_type: str,
        key_prefix: str = "ai_interp"
    ):
        """AIè§£é‡ˆã‚’è¡¨ç¤ºã™ã‚‹å…±é€šé–¢æ•°"""
        if not enabled or not api_key:
            return

        st.subheader(f"ğŸ¤– AIçµ±è¨ˆè§£é‡ˆ")

        interpretation_key = f"{key_prefix}_interpretation"

        # è§£é‡ˆãƒœã‚¿ãƒ³
        if st.button(f"çµ±è¨ˆçµæœã‚’è§£é‡ˆã™ã‚‹", key=f"{key_prefix}_button"):
            with st.spinner("AIãŒçµ±è¨ˆçµæœã‚’åˆ†æä¸­..."):
                # åˆ†æã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
                if analysis_type == 'correlation':
                    prompt = AIStatisticalInterpreter.create_correlation_interpretation_prompt(results)
                elif analysis_type == 'chi_square':
                    prompt = AIStatisticalInterpreter.create_chi_square_interpretation_prompt(results)
                elif analysis_type == 'ttest':
                    prompt = AIStatisticalInterpreter.create_ttest_interpretation_prompt(results)
                elif analysis_type == 'anova':
                    prompt = AIStatisticalInterpreter.create_anova_interpretation_prompt(results)
                else:
                    st.error("æœªå¯¾å¿œã®åˆ†æã‚¿ã‚¤ãƒ—ã§ã™ã€‚")
                    return

                # APIå‘¼ã³å‡ºã—
                interpretation = AIStatisticalInterpreter.call_gemini_api(api_key, prompt)

                # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                st.session_state[interpretation_key] = interpretation

        # è§£é‡ˆçµæœãŒã‚ã‚‹å ´åˆã¯å¸¸ã«è¡¨ç¤º
        if interpretation_key in st.session_state:
            st.markdown("### ğŸ“Š çµ±è¨ˆè§£é‡ˆçµæœ")
            st.write(st.session_state[interpretation_key])

            # è§£é‡ˆã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ãƒœã‚¿ãƒ³
            col1, col2 = st.columns([1, 1])
            with col2:
                if st.button(f"è§£é‡ˆã‚’ã‚¯ãƒªã‚¢", key=f"{key_prefix}_clear"):
                    del st.session_state[interpretation_key]
                    st.rerun()
